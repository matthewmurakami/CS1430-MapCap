%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% CSCI 1430 Project Progress Report 1 Template
%
% This is a LaTeX document. LaTeX is a markup language for producing documents.
% Your task is to answer the questions by filling out this document, then to 
% compile this into a PDF document. 
% You will then upload this PDF to `Gradescope' - the grading system that we will use. 
% Instructions for upload will follow soon.
%
% 
% TO COMPILE:
% > pdflatex thisfile.tex
%
% If you do not have LaTeX and need a LaTeX distribution:
% - Departmental machines have one installed.
% - Personal laptops (all common OS): http://www.latex-project.org/get/
%
% If you need help with LaTeX, come to office hours. Or, there is plenty of help online:
% https://en.wikibooks.org/wiki/LaTeX
%
% Good luck!
% James and the 1430 staff
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% How to include two graphics on the same line:
% 
% \includegraphics[width=0.49\linewidth]{yourgraphic1.png}
% \includegraphics[width=0.49\linewidth]{yourgraphic2.png}
%
% How to include equations:
%
% \begin{equation}
% y = mx+c
% \end{equation}
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[colorlinks = true,
            linkcolor = blue,
            urlcolor  = blue]{hyperref}
\usepackage[a4paper,margin=1.5in]{geometry}
\usepackage{stackengine,graphicx}
\usepackage{fancyhdr}
\setlength{\headheight}{15pt}
\usepackage{microtype}
\usepackage{times}
\usepackage{booktabs}

% From https://ctan.org/pkg/matlab-prettifier
\usepackage[numbered,framed]{matlab-prettifier}

\frenchspacing
\setlength{\parindent}{0cm} % Default is 15pt.
\setlength{\parskip}{0.3cm plus1mm minus1mm}

\pagestyle{fancy}
\fancyhf{}
\lhead{Final Project Progress Report 1}
\rhead{CSCI 1430}
\rfoot{\thepage}

\date{}

\title{\vspace{-1cm}Final Project Progress Report 1}

\begin{document}
\maketitle
\vspace{-1cm}
\thispagestyle{fancy}

\textbf{Team name: \emph{MapCap}}\\
\textbf{TA name: \emph{Srinath Sridhar}}

\emph{Note:} when submitting this document to Gradescope, make sure to add all other team members to the submission. This can be done on the submission page after uploading.

\section*{Progress Report}

\paragraph{Restate the goal of the project succinctly.}
The goal of our project is to augment attribution methods for CLIP zero-shot image classification using CLIPCap. This is so that we are able to understand why a model classifies something the way it does by making attribution maps more interpretable through the use language models. The language model will be used to describe the regions of images that an attribution method highlights across an entire class. We will then summarize all of the descriptions such that a human will be able to quickly understand how CLIP is making its decisions. Afterward, we will analyze trends between classification performance and properties of the patch caption summaries. 

\paragraph{What has the team collectively accomplished?}
So far, our team has collectively made significant progress towards our goal. We were able to get access to our required data by taking a partition of TinyImageNet (5 classes, 10 images per class) so that we could run our pre-trained model on a smaller sample size. Additionally, we ran inference over CLIP for zero-shot image classification over this partition (See Table\ref{perfTable}). We experimented with gradient-based saliency maps and found that ViT feature extraction renders them difficult to interpret (See Figure \ref{saliency}). From there, we explored more advanced attribution methods, settling on an attention-based attribution method presented in \textit{Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers}. We then ran this method on two images that were misclassified in the zero shot setting (see Figure \ref{Map}). Finally, we ran CLIPCap on these same two images, as well as a third image that was correctly classified (See Figure \ref{Cap}) Both the CLIPCap and the Attention explainability method papers provide Google Colaboratory notebooks, which we leverage.


\paragraph{What individual tasks have been accomplished?}
In terms of individual tasks that have been accomplished, one team member created a script that will partition Tiny Imagenet while another member ran inference on the data using the CLIP model. Finally, the last member created and visualized the saliency map for each image and generated initial data using CLIPCap and the attention-based attribution method. 

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{SlugSaliency.png}
    \caption{TinyImageNet image of a slug, and it's corresponding gradient based saliency map. Note how the center of every ViT patch is considered salient.}
    \label{saliency}
\end{figure}

\begin{table}[h]
    \centering
    \begin{tabular}{lr}
        \toprule
        Class & Accuracy\\
        \midrule
        Slug & 90\% \\
        Lawn Mower & 100\% \\
        Pill Bottle & 90\% \\
        Goldfish & 100\% \\
        Birdhouse & 100\% \\
        \bottomrule
    \end{tabular}
    \label{perfTable}

\end{table}

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{SlugMap.png}
    \includegraphics[width=10cm]{PillBottleMap.png}
    \caption{This attention-based attribution method computes an attribution map based on an image and the corresponding text. (Top) Image of a slug that was misclassified as a goldfish. Attribution map generated for both the correct class and the incorrect class. (Bottom) Image corresponding to the label ``Pill Bottle'' that was misclassified as a bird house.}
    \label{Map}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=10cm]{SlugClipCap.png}
    \includegraphics[width=10cm]{PillBottleClipCap.png}
    \includegraphics[width=10cm]{BirdhouseClipCap.png}
    \caption{CLIPCap captions for three images. (Top) Slug, misclassified as goldfish. (Middle) Pill Bottle, misclassified as birdhouse. (Bottom) Birdhouse, correctly classified.}
    \label{Cap}
\end{figure}

\paragraph{What are the current tasks?}
Our current task is to implement segmentation over the heatmap generated by the attention-based attribution method, generate image representations for each segment, and feed them into CLIPCap.

\paragraph{What tasks remain undefined?}
We currently do not have a good way to quantify our natural-language explainability method's usefulness.

\paragraph{What are the next steps?}
After the current task is completed, we must implement unsupervised extractive summarization to create a simple description of the features used by the CLIP. Finally, we must increase our dataset size, and rewrite the CLIPCap and Attention Attribution method Colaboratory notebooks to run over our sequences of images.

\paragraph{Are you missing resources? Data, compute, skills?}
We have all resources that we need.

\end{document}